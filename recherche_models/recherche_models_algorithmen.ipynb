{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":14436,"status":"ok","timestamp":1683987646873,"user":{"displayName":"Susi H","userId":"16263607694578559646"},"user_tz":-120},"id":"ZISDJ2NTnIXx","outputId":"ea140451-00cb-45e3-a0b6-3b417a19e835"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1bce8cc7-af20-4ef1-b664-ccbe20488d89\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1bce8cc7-af20-4ef1-b664-ccbe20488d89\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Symptom2Disease.csv to Symptom2Disease.csv\n"]}],"source":["#Nur für Colab\n","#Datei hochladen\n","from google.colab import files\n","\n","uploaded = files.upload()"],"id":"ZISDJ2NTnIXx"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":76},"executionInfo":{"elapsed":1546436,"status":"ok","timestamp":1683623821551,"user":{"displayName":"Billy Andersson","userId":"04670995206192713080"},"user_tz":-120},"id":"RH8D8jaSuSM6","outputId":"40f338ff-3073-411a-d8d3-98458aefa3ec"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-dc66160a-1323-4b93-a40d-38ec6838dc11\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-dc66160a-1323-4b93-a40d-38ec6838dc11\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Data1.csv to Data1.csv\n"]}],"source":["#Für Billy\n","from google.colab import files\n","\n","billys_csv = files.upload()"],"id":"RH8D8jaSuSM6"},{"cell_type":"markdown","metadata":{"id":"b6a8812b"},"source":["# 1 Problemstellung\n","\n","- Problemart: supervised Multi-Class Single-Label Text Classification\n","- Supervised = Model wird mit Datensätzen trainiert, bei denen das Label bereits bekannt ist\n","- Classification = Zuordnung zu diskreten Werten\n","- Multiclass Single-Label = mehrere Labels, jedem Sample kann nur ein Label zugewiesen werden\n","\n","Andere Arten:\n","- Multi-Label Classification (mehrere Labels, jedem Sampel können mehrere Labels zugewiesen werden)\n","- Binary Classification (z.B. Sentiment-Analysis, nur zwei Label, z.B. positiv und negativ)"],"id":"b6a8812b"},{"cell_type":"markdown","metadata":{"id":"2ec2b10d"},"source":["# 2 Methoden\n","_https://en.wikipedia.org/wiki/Multiclass_classification_<br>\n","_https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=a546f2c88c588a2a46c054f67b39a3ebefdae694_<br>\n","_https://www.ijcsit.com/docs/Volume%204/Vol4Issue4/ijcsit2013040408.pdf_"],"id":"2ec2b10d"},{"cell_type":"markdown","metadata":{"id":"a29e91a9"},"source":["## 2.1 Umwandeln in binäre Klassifikation\n","= Problem wird auf mehrere binäre Klassifikationen zurückgeführt"],"id":"a29e91a9"},{"cell_type":"markdown","metadata":{"id":"9d5489ba"},"source":["### 2.1.1 One-vs.-all/One-vs.-rest\n","_https://youtu.be/ZvaELFv5IpM_\n","<figure>\n","    <img src=\"https://www.cc.gatech.edu/classes/AY2016/cs4476_fall/results/proj4/html/jnanda3/svm.png\" alt=\"One-vs.-all Algorithmus\" style=\"width: 400px;\"/>\n","    <br>\n","</figure>\n","<small>Quelle: https://www.cc.gatech.edu/classes/AY2016/cs4476_fall/results/proj4/html/jnanda3/index.html</small>\n","\n","- es wird pro Klasse ein binärer Classifier (z.B. eine logistische Regression oder SVM) trainiert nach dem Prinzip \"einer gegen alle\"\n","    - Beispiel: rot, grün, blau und gelb\n","        - rot vs. nicht rot\n","        - grün vs. nicht grün\n","        - blau vs. nicht blau\n","        - gelb vs. nicht gelb\n","- dann werden auf jeden Punkt im Datensatz alle Classifier angewendet\n","- der Classifier, der die größte Confidence hat, wird gewählt, um den Punkt zu klassifizieren\n","- wird bei großer Anzahl an Klassen ineffizient"],"id":"9d5489ba"},{"cell_type":"markdown","metadata":{"id":"64e58da3"},"source":["### 2.1.2 One-vs.-one\n","_https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/_\n","<figure>\n","    <img src=\"https://image.slidesharecdn.com/linearmodelsandmulticlassclassification2-170312171304/75/linear-models-and-multiclass-classification-25-2048.jpg?cb=1667658416\" alt=\"One-vs.-one Algorithmus\" style=\"width: 400px;\"/>\n","    <br>\n","</figure>\n","<small>Quelle: https://www.slideshare.net/Paxcel/binary-and-multi-class-strategies-for-machine-learning</small>\n","\n","- Problem wird in binäre Classifier aufgeteilt nach dem Prinzip \"jede Klasse gegen jede\"\n","    - Beispiel: rot, grün, blau und gelb\n","        - rot vs. grün\n","        - rot vs. blau\n","        - rot vs. gelb\n","        - grün vs. blau\n","        - grün vs. gelb\n","        - blau vs. gelb\n","- jeder Classifier wird nur mit den Datensätzen trainiert, die einer der beiden Klassen angehören\n","- Anzahl der binären Klassifikationsprobleme: (N * (N-1))/2\n","- nach Scoring-System wird am Ende eine Klasse bestimmt"],"id":"64e58da3"},{"cell_type":"markdown","metadata":{"id":"e096403e"},"source":["## 2.2 Erweitern der binären Klassifikation\n","= Classifier, die eigentlich für binäre Klassifikation gedacht sind, aber für multiclass erweitert werden"],"id":"e096403e"},{"cell_type":"markdown","metadata":{"id":"048b8a28"},"source":["### 2.2.1 Neuronales Netz\n","_https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/video-lecture_\n","- Neuronales Netz nach One-vs.-all: \n","    - NN für binäre Klassifikation hat üblicherweise einen Output-Knoten, der binär klassifiziert, stattdessen neuronales Netz mit so vielen Output-Knoten, wie es Klassen gibt\n","    - die Wahrscheinlichkeiten der Output-Knoten sollen addiert 1 ergeben (-> Softmax als Loss-Funktion/Optimierungsfunktion)\n","- Nachteil: verlangt sehr viel mehr Rechenzeit und -power als andere Herangehensweisen"],"id":"048b8a28"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"OiT4nkJxJtVX","outputId":"b0a53aab-7f2d-483e-9175-f844f9c71c9b","executionInfo":{"status":"error","timestamp":1683623830293,"user_tz":-120,"elapsed":404,"user":{"displayName":"Billy Andersson","userId":"04670995206192713080"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-b719fc941c6d>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data1.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'uploaded' is not defined"]}],"source":["import pandas as pd\n","import io\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, LSTM\n","\n","# Load the dataset\n","data = pd.read_csv(io.BytesIO(billys_csv['Data1.csv']))\n","data.pop(\"id\")\n","\n","# Convert diagnosis column to integers\n","diagnosis_dict = {diagnosis: i for i, diagnosis in enumerate(data['label'].unique())}\n","data['label'] = data['label'].apply(lambda x: diagnosis_dict[x])\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n","\n","# Convert the diagnosis labels into a one-hot encoding format\n","num_classes = len(data['label'].unique())\n","y_train = np.eye(num_classes)[y_train]\n","y_test = np.eye(num_classes)[y_test]\n","\n","# Tokenize the symptoms and convert them into sequences\n","max_words = 10000\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(X_train)\n","X_train_sequences = tokenizer.texts_to_sequences(X_train)\n","X_test_sequences = tokenizer.texts_to_sequences(X_test)\n","\n","# Pad the sequences to ensure that they are all the same length\n","max_len = 100\n","X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len)\n","X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)\n","\n","# Build the neural network model\n","embedding_size = 128\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_size, input_length=max_len))\n","model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))  #bei mehreren Layers noch zusätzlich return_sequences=True eintragen\n","#model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","batch_size = 32\n","epochs = 10\n","model.fit(X_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_padded, y_test))\n","\n","# Evaluate the model on the testing set\n","loss, accuracy = model.evaluate(X_test_padded, y_test, batch_size=batch_size)\n","print('Test loss:', loss)\n","print('Test accuracy:', accuracy)"],"id":"OiT4nkJxJtVX"},{"cell_type":"markdown","metadata":{"id":"fa5b2656"},"source":["### 2.2.2 K-nearest-neighbor\n","_https://towardsdatascience.com/multiclass-classification-using-k-nearest-neighbours-ca5281a9ef76_\n","- Abstand des zu klassifizierenden Punktes zu allen seinen Nachbarn wird berechnet\n","- Nachbarn werden nach Abstand sortiert\n","- Punkt wird in die Klasse eingeordnet, der die meisten seiner k-nächsten Nachbarn angehören\n","- Funktioniert besser bei kleinen Datensätzen, wird sonst ineffizient"],"id":"fa5b2656"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3794,"status":"ok","timestamp":1683970328000,"user":{"displayName":"Susi H","userId":"16263607694578559646"},"user_tz":-120},"id":"0256be75","outputId":"12de932a-f7b5-473c-c88f-76af2278cca1"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                 precision    recall  f1-score   support\n","\n","                           Acne       1.00      1.00      1.00        16\n","                      Arthritis       0.91      1.00      0.95        20\n","               Bronchial Asthma       1.00      0.81      0.90        16\n","           Cervical spondylosis       0.95      1.00      0.98        20\n","                    Chicken pox       0.64      0.93      0.76        15\n","                    Common Cold       0.93      1.00      0.97        14\n","                         Dengue       0.71      0.89      0.79        19\n","          Dimorphic Hemorrhoids       1.00      1.00      1.00        19\n","               Fungal infection       0.95      1.00      0.98        21\n","                   Hypertension       0.92      0.92      0.92        13\n","                       Impetigo       1.00      0.83      0.91        12\n","                       Jaundice       0.92      1.00      0.96        11\n","                        Malaria       0.90      1.00      0.95        18\n","                       Migraine       0.95      1.00      0.97        18\n","                      Pneumonia       0.94      1.00      0.97        17\n","                      Psoriasis       0.94      0.75      0.83        20\n","                        Typhoid       0.79      0.55      0.65        20\n","                 Varicose Veins       1.00      0.92      0.96        12\n","                        allergy       0.94      0.88      0.91        17\n","                       diabetes       0.92      1.00      0.96        12\n","                  drug reaction       1.00      0.77      0.87        13\n","gastroesophageal reflux disease       0.95      0.91      0.93        22\n","           peptic ulcer disease       0.93      0.93      0.93        14\n","        urinary tract infection       1.00      0.88      0.94        17\n","\n","                       accuracy                           0.91       396\n","                      macro avg       0.92      0.92      0.92       396\n","                   weighted avg       0.92      0.91      0.91       396\n","\n"]}],"source":["#Vor dem Ausführen der Zelle muss ganz oben einmal die csv-Datei hochgeladen werden!\n","#Beispiel-Implementierung von K-nearest-neighbor\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import metrics\n","import io\n","\n","#Datei einlesen\n","data = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","data.pop(\"id\")\n","data = data.sample(frac = 1)\n","\n","#Aufteilen in Trainings- und Testdaten\n","X = data[\"text\"]\n","y = data[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n","\n","#Vektorisieren\n","cv = CountVectorizer()\n","X_train_counts = cv.fit_transform(X_train)\n","X_test_counts = cv.transform(X_test)\n","\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n","\n","#Trainieren\n","knn = KNeighborsClassifier()\n","\n","knn.fit(X_train_tfidf, y_train)\n","\n","#Vorhersagen der Testdaten\n","predicted = knn.predict(X_test_tfidf)\n","\n","#Auswertung\n","print(metrics.classification_report(y_test, predicted))"],"id":"0256be75"},{"cell_type":"markdown","metadata":{"id":"1eff4278"},"source":["### 2.2.3 Naive Bayes\n","_https://youtu.be/EGKeC2S44Rs_\n","- Familie von Algorithmen\n","- Stellen die \"naive\" Annahme, dass die Wörter alle voneinander unabhängig sind\n","- Ablauf:\n","    - Datensätze werden vektorisiert (Count Vectorizer):\n","        - Länge des Vektors = Anzahl der Worte im Vokabular\n","        - Vektor drückt aus, wie oft jedes Wort des Vokabulars im Datensatz enthalten ist\n","    - Aus den Trainingsdaten wird die Wahrscheinlichkeit für jede Klasse abgeleitet\n","    - für jede Klasse wird die Wahrscheinlichkeit von jedem Wort berechnet"],"id":"1eff4278"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":331,"status":"ok","timestamp":1682932927193,"user":{"displayName":"Susi H","userId":"16263607694578559646"},"user_tz":-120},"id":"600e06f0","outputId":"02848d89-6eb9-423d-bb1b-cf1202f986b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 precision    recall  f1-score   support\n","\n","                           Acne       1.00      1.00      1.00        18\n","                      Arthritis       0.82      1.00      0.90        14\n","               Bronchial Asthma       0.89      1.00      0.94        16\n","           Cervical spondylosis       1.00      0.94      0.97        17\n","                    Chicken pox       1.00      0.94      0.97        18\n","                    Common Cold       0.94      0.94      0.94        18\n","                         Dengue       0.86      0.86      0.86        14\n","          Dimorphic Hemorrhoids       0.88      1.00      0.93        14\n","               Fungal infection       0.74      1.00      0.85        14\n","                   Hypertension       1.00      1.00      1.00        12\n","                       Impetigo       0.88      1.00      0.93        14\n","                       Jaundice       0.95      1.00      0.97        19\n","                        Malaria       0.90      1.00      0.95        18\n","                       Migraine       1.00      0.95      0.97        19\n","                      Pneumonia       1.00      0.92      0.96        24\n","                      Psoriasis       0.93      0.68      0.79        19\n","                        Typhoid       0.88      1.00      0.93        14\n","                 Varicose Veins       1.00      1.00      1.00        19\n","                        allergy       1.00      0.75      0.86        20\n","                       diabetes       1.00      0.40      0.57        20\n","                  drug reaction       1.00      0.69      0.81        16\n","gastroesophageal reflux disease       0.75      1.00      0.86        12\n","           peptic ulcer disease       0.44      0.78      0.56         9\n","        urinary tract infection       0.90      1.00      0.95        18\n","\n","                       accuracy                           0.90       396\n","                      macro avg       0.91      0.91      0.90       396\n","                   weighted avg       0.92      0.90      0.90       396\n","\n"]}],"source":["#Vor dem Ausführen muss ganz oben einmal die Datei hochgeladen werden!\n","#Beispiel für Implementierung von Naive Bayes Classifier\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn import metrics\n","import io\n","\n","#Datei einlesen\n","data = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","data.pop(\"id\")\n","data = data.sample(frac = 1)\n","\n","#Aufteilen in Trainings- und Testdaten\n","X = data[\"text\"]\n","y = data[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n","\n","#Pipeline erstellen mit Vektorisierung, Transformation und Naive Bayes Classifier\n","text_clf = Pipeline([('vect', CountVectorizer()),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', MultinomialNB()),\n","                     ])\n","\n","#Trainieren\n","text_clf.fit(X_train, y_train)\n","\n","#Vorhersagen der Testdaten\n","predicted = text_clf.predict(X_test)\n","\n","#Auswertung\n","print(metrics.classification_report(y_test, predicted))"],"id":"600e06f0"},{"cell_type":"markdown","metadata":{"id":"1eedbea5"},"source":["### 2.2.4 Support Vector Machine\n","_https://www.baeldung.com/cs/svm-multiclass-classification#multiclass-classification-using-svm_\n","- Ursprünglich designed für binäre Klassifikation, aber auch anwendbar auf Multi-Class Probleme\n","- Grundsätzliche Vorgehensweise bei binärer Klassifikation:\n","    - Ziel: Linie (bzw. Ebene) finden, die die Punkte der beiden Klassen am besten trennt (maximiert den Abstand zu den Punkten beider Klassen)\n","    - Die Datensätze, die der Linie am nächsten sind, sind die Support-Vectors\n","    - Abstand der Support-Vectors zur Linie ist die Margin, die maximiert werden soll\n","    - Die Berechnung der Trennlinie wird mit eine Kernel-Funktion durchgeführt\n","    - Es gibt verschiedene Kernel-Funktionen, deren Hyperparameter beim Tuning angepasst werden\n","- Zwei Herangehensweisen: \n","    - One-to-one\n","    - One-to-rest\n","- Verfügbar in sklearn\n","- Vorteile:\n","    - Speichereffizient\n","    - anpassbar (spezifische Kernel Functions auswählbar)\n","    - kommt mit kleinem Trainigsdatensatz aus"],"id":"1eedbea5"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1682933404068,"user":{"displayName":"Susi H","userId":"16263607694578559646"},"user_tz":-120},"id":"D77d4vvGoPTQ","outputId":"11ebe2ea-29bc-4756-b424-24387e791dc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 precision    recall  f1-score   support\n","\n","                           Acne       1.00      1.00      1.00        17\n","                      Arthritis       1.00      1.00      1.00        17\n","               Bronchial Asthma       1.00      1.00      1.00        11\n","           Cervical spondylosis       1.00      1.00      1.00        21\n","                    Chicken pox       1.00      0.93      0.96        14\n","                    Common Cold       0.95      1.00      0.97        18\n","                         Dengue       0.87      1.00      0.93        13\n","          Dimorphic Hemorrhoids       1.00      1.00      1.00        18\n","               Fungal infection       1.00      1.00      1.00        15\n","                   Hypertension       1.00      1.00      1.00        15\n","                       Impetigo       1.00      1.00      1.00        23\n","                       Jaundice       1.00      1.00      1.00        15\n","                        Malaria       1.00      1.00      1.00        14\n","                       Migraine       1.00      1.00      1.00        17\n","                      Pneumonia       1.00      1.00      1.00        18\n","                      Psoriasis       1.00      0.79      0.88        14\n","                        Typhoid       0.94      0.94      0.94        17\n","                 Varicose Veins       0.92      1.00      0.96        12\n","                        allergy       1.00      0.95      0.97        20\n","                       diabetes       0.83      1.00      0.91        15\n","                  drug reaction       1.00      0.82      0.90        17\n","gastroesophageal reflux disease       1.00      1.00      1.00        19\n","           peptic ulcer disease       0.93      1.00      0.96        13\n","        urinary tract infection       1.00      1.00      1.00        23\n","\n","                       accuracy                           0.98       396\n","                      macro avg       0.98      0.98      0.97       396\n","                   weighted avg       0.98      0.98      0.98       396\n","\n"]}],"source":["#Vor dem Ausführen muss ganz oben einmal die Datei hochgeladen werden!\n","#Beispiel für die Implementierung von linear Support-Vector-Machine\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.svm import LinearSVC\n","import io\n","\n","#Datei einlesen\n","data = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","data.pop(\"id\")\n","data = data.sample(frac = 1)\n","\n","#Aufteilen in Trainings- und Testdaten\n","X = data[\"text\"]\n","y = data[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n","\n","#Pipeline mit Vektorisierung, Transformation und SVM\n","text_clf = Pipeline([('vect', CountVectorizer()),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', LinearSVC()),\n","                     ])\n","\n","#Trainieren\n","text_clf.fit(X_train, y_train)\n","\n","#Vorhersagen der Testdaten\n","predicted = text_clf.predict(X_test)\n","\n","#Auswertung\n","print(metrics.classification_report(y_test, predicted))"],"id":"D77d4vvGoPTQ"},{"cell_type":"markdown","metadata":{"id":"27942b69"},"source":["### 2.2.5 Decision Trees\n","_https://www.youtube.com/watch?v=ZVR2Way4nwQ_\n","- Model entwickelt einen Entscheidungsbaum anhand der Trainingsdaten\n","- Aufbau des Entscheidungsbaums:\n","    - Alle Datensätze sind in der Root\n","    - Alle Blätter enthalten nur noch Datensätze von einer Klasse\n","    - Model vergleicht alle möglichen Aufteilungen (Splits)\n","    - Model sucht Split so aus, dass die Entropie in einem Kindknoten möglichst gering ist -> und der Informationgain möglichst groß ist\n","- neue Datensätze werden mit Hilfe des Entscheidungsbaums in die entsprechende Klasse eingeordnet"],"id":"27942b69"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":576,"status":"ok","timestamp":1682938351477,"user":{"displayName":"Susi H","userId":"16263607694578559646"},"user_tz":-120},"id":"jPXe6iRZ7iv0","outputId":"f669fac1-2fb4-4cec-86e0-8140d13e16a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 precision    recall  f1-score   support\n","\n","                           Acne       1.00      1.00      1.00        24\n","                      Arthritis       1.00      1.00      1.00        14\n","               Bronchial Asthma       0.85      0.85      0.85        20\n","           Cervical spondylosis       0.92      0.92      0.92        13\n","                    Chicken pox       0.27      0.19      0.22        16\n","                    Common Cold       0.52      0.79      0.63        14\n","                         Dengue       0.36      0.50      0.42        16\n","          Dimorphic Hemorrhoids       1.00      1.00      1.00        15\n","               Fungal infection       0.92      0.67      0.77        18\n","                   Hypertension       1.00      0.73      0.84        11\n","                       Impetigo       0.76      0.76      0.76        17\n","                       Jaundice       1.00      1.00      1.00        16\n","                        Malaria       1.00      0.94      0.97        17\n","                       Migraine       1.00      0.79      0.88        19\n","                      Pneumonia       0.93      0.72      0.81        18\n","                      Psoriasis       0.46      0.46      0.46        13\n","                        Typhoid       0.68      0.93      0.79        14\n","                 Varicose Veins       1.00      0.96      0.98        24\n","                        allergy       0.61      0.69      0.65        16\n","                       diabetes       0.73      0.58      0.65        19\n","                  drug reaction       0.44      0.64      0.52        11\n","gastroesophageal reflux disease       0.62      0.72      0.67        18\n","           peptic ulcer disease       0.79      0.88      0.83        17\n","        urinary tract infection       0.92      0.75      0.83        16\n","\n","                       accuracy                           0.78       396\n","                      macro avg       0.78      0.77      0.77       396\n","                   weighted avg       0.80      0.78      0.78       396\n","\n"]}],"source":["#Vor dem Ausführen muss ganz oben die Datei einmal hochgeladen werden!\n","#Beispiel für die Implementierung von Decision Trees\n","import pandas as pd\n","import io\n","from sklearn.model_selection import train_test_split\n","from sklearn import tree\n","from sklearn.pipeline import Pipeline\n","from sklearn import metrics\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","#Datei einlesen\n","data = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","data.pop(\"id\")\n","data = data.sample(frac = 1)\n","\n","#Aufteilen in Trainings- und Testdaten\n","X = data[\"text\"]\n","y = data[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n","\n","#Pipeline mit Vektorisierung, Transformation und Decision Tree\n","text_clf = Pipeline([('vect', CountVectorizer()),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', tree.DecisionTreeClassifier()),\n","                     ])\n","\n","#Trainieren\n","text_clf.fit(X_train, y_train)\n","\n","#Vorhersagen der Testdaten\n","predicted = text_clf.predict(X_test)\n","\n","#Auswertung\n","print(metrics.classification_report(y_test, predicted))"],"id":"jPXe6iRZ7iv0"},{"cell_type":"markdown","source":["###2.2.6 Gradient Boost\n","\n","https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n","\n","* basiert auf Ensemble Methoden\n","* kombiniert mehrere schwache Lernalgorithmen zu einer starken; jeder Algorithmus wird nacheinander trainiert und versucht den schwächeren zu verbessern\n","\n","\n","*   beginnt mit Basismodel von beliebigen Algorithmus, der für Klassifikation oder Regression geeignet ist\n","    \n","    - neues Model wird erstellt und korrigiert die Fehler des Basismodels; Gradientenabstieg wird verwendet um das neue Modell zu trainieren\n","\n","    - neues trainiertes Modell wird dem Basismodell hinzugefügt, um ein neues Essemble Modell zu erstellen -> das wird oft wiederholt\n","  \n","\n","\n","*   sehr leistungsfähiger Algorithmus und gut für große Datensätze, vor Allem weil das Modell durch die ganzen Wiederholungen sehr präzise wird\n","\n","    - ist aber auch sehr rechenintensiv\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"P4H47EIVnXVo"},"id":"P4H47EIVnXVo"},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import io\n","\n","# Einlesen des Datensatzes als CSV-Datei\n","df = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","df.pop(\"id\")\n","\n","# Aufteilen der Features und Labels\n","X = df['text']\n","y = df['label']\n","\n","# Aufteilen des Datensatzes in Trainings- und Testdaten\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","#Pipeline mit Vektorisierung, Transformation und Gradient Boost Classifier\n","text_clf = Pipeline([('vect', CountVectorizer()),\n","                     ('gb_clf', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)),\n","                     ])\n","\n","# Trainieren des Modells\n","text_clf.fit(X_train, y_train)\n","\n","# Vorhersage auf Testdaten machen\n","y_pred = text_clf.predict(X_test)\n","\n","# Berechnen der Genauigkeit\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"id":"f-5KQRytreiV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683574195805,"user_tz":-120,"elapsed":14339,"user":{"displayName":"Susi H","userId":"16263607694578559646"}},"outputId":"b1455d07-636d-4d1c-9037-f61925d5af9a"},"id":"f-5KQRytreiV","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.89\n"]}]},{"cell_type":"markdown","metadata":{"id":"6a40b7b7"},"source":["## 2.3 Hierarchische Klassifikation\n","= Aufteilung in Ober- und Unterklassen in einem Baum, für die Aufteilung an den einzelnen Knoten wird meist ein binärer Classifier genutzt"],"id":"6a40b7b7"},{"cell_type":"markdown","source":["# 3 Persistenz\n","*https://scikit-learn.org/stable/model_persistence.html*\n","- wichtig bei Text-Klassifizierung: sowohl das Model, als auch der/die Vectorizer müssen gespeichert werden\n","- es muss außerdem sichergestellt werden, dass das Model in die gleiche scikit-learn Version geladen wird, in der es auch gespeichert wurde"],"metadata":{"id":"IOFLFkcOYc__"},"id":"IOFLFkcOYc__"},{"cell_type":"markdown","source":["## 3.1 Python-spezifische Serialisierung\n","\n","- unterstützen alle auch das Speichern gesamter Pipelines"],"metadata":{"id":"QndeWrAdjrTP"},"id":"QndeWrAdjrTP"},{"cell_type":"markdown","source":["### 3.1.1 Speichern mit Pickle"],"metadata":{"id":"JKOHrw5PZFoD"},"id":"JKOHrw5PZFoD"},{"cell_type":"code","source":["#In colab wird das Model nur in dem Sitzungsspeicher abgespeichert, also ist das Model nur für die aktuelle Sitzung persistent (kann aber natürlich aus Colab runtergeladen oder ein anderes hochgeladen werden)\n","#Aber wenn man es in JupyterNotebook o.Ä. benutzt wird es lokal im selben Verzeichnis gespeichert\n","import pickle\n","\n","#Speichern\n","with open('file.pkl', 'wb') as f:\n","  pickle.dump(model, f)\n","\n","#Laden\n","with open('file.pkl', 'rb') as f:\n","    loaded_model = pickle.load(f)"],"metadata":{"id":"oFdm2NaYavHy"},"id":"oFdm2NaYavHy","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.1.2 Speichern mit joblib\n","\n","- effizienter als Pickle, besonders bei großen Arrays"],"metadata":{"id":"2DQMp1YjZLnU"},"id":"2DQMp1YjZLnU"},{"cell_type":"code","source":["from joblib import dump, load\n","\n","#Speichern\n","dump(model, 'file.joblib')\n","\n","#Laden\n","loaded_model = load('file.joblib')"],"metadata":{"id":"bjxVSw10d_XO"},"id":"bjxVSw10d_XO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.1.3 Speichern mit skops\n","*https://skops.readthedocs.io/en/stable/persistence.html*\n","\n","- sicherer als joblib und Pickle\n","- lädt nur Models, die keine \"untrusted types\" enthalten"],"metadata":{"id":"hM56PEY6ffvy"},"id":"hM56PEY6ffvy"},{"cell_type":"code","source":["import skops.io as sio\n","\n","#Speichern\n","sio.dump(model, 'file.skops')\n","\n","#Überprüfung der untrusted types\n","unknown_types = sio.get_untrusted_types(file=\"file.skops\")\n","print(unknown_types)\n","\n","#Laden des Models nach Überprüfung der untrusted types\n","loaded_model = sio.load(\"file.skops\", trusted=unknown_types)"],"metadata":{"id":"TSb2ly8-f5qW"},"id":"TSb2ly8-f5qW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 Kompatible Formate\n","\n","- wenn ein Model über unterschiedliche Architekturen und Umgebungen hinaus genutzt werden soll, sollte das Model am besten in eines der \"interoperable formats\" exportiert werden\n","- z.B. wenn die Prediction in einer anderen Umgebung gemacht werden soll als das Training"],"metadata":{"id":"42Kx-QCVj_PA"},"id":"42Kx-QCVj_PA"},{"cell_type":"markdown","source":["### 3.2.1 Open Neural Network Exchange (ONNX)\n","*http://onnx.ai/sklearn-onnx/*\n","- binäre Serialisierung des Models\n","- Vorgaben/Einschränkungen:\n","  - Für das Konvertieren muss der vektorisierte Datensatz von einer \"sparse matrix\" in eine \"dense matrix\" umgewandelt werden.\n","  - Nachteil dabei ist, dass sparse matrices effizienter (besonders was den Speicher angeht) sind als dense matrices.\n","  - Außerdem kann das Konvertieren unter Umständen zu einem Verlust bei der accuracy führen, wird aber bei unserer Problemstellung und unseren Models vermutlich kaum eine Rolle spielen.\n","  - Kann aber ausgeglichen werden, indem das Model auf die Verwendung von dense matrices getuned wird\n","- Alle Schritte von Vektorisierung bis eigentliches Model müssen für ein beständiges Output konvertiert werden\n","  - dabei ist es leider nicht so einfach, die alle in einem Model zu speichern\n","  - deshalb müssen sie vermutlich alle einzeln konvertiert werden, das bedeutet, dass besonders sorgfältig darauf geachtet werden muss, dass Reihenfolge und Formate nach dem Laden eingehalten werden\n","- Das Konvertieren von gesamten Pipelines ist auch möglich, aber dann ist meist immer noch eine Umwandlung der sparse Matrix in eine dense Matrix nötig, was bei einer Pipeline nur schwer umsetzbar ist\n"],"metadata":{"id":"dJUkCcUhk7dF"},"id":"dJUkCcUhk7dF"},{"cell_type":"code","source":["from onnx import helper, TensorProto, numpy_helper\n","import onnx\n","import skl2onnx\n","import onnxruntime as rt\n","from onnxconverter_common.data_types import StringTensorType, Int64TensorType, FloatTensorType\n","\n","#Beispiel für das Transformieren eines Count Vectorizers in ein dense numpy array\n","X_train_np = X_train_tfidf.toarray()\n","X_test_np = X_test_tfidf.toarray()\n","\n","#Vectorizer konvertieren\n","initial_type = [('input', StringTensorType([None,]))]\n","onnx_cv = skl2onnx.convert_sklearn(cv, initial_types=initial_type)\n","\n","#Transformer konvertieren\n","initial_type = [('input', FloatTensorType([None, X_train_np.shape[1]]))]\n","onnx_tfidf = skl2onnx.convert_sklearn(tfidf_transformer, initial_types=initial_type)\n","\n","#Model konvertieren\n","initial_type = [('input', FloatTensorType([None, X_train_np.shape[1]]))]\n","onnx_model = skl2onnx.convert_sklearn(model, initial_types=initial_type)\n","\n","#konvertierte Models speichert\n","onnx.save_model(onnx_cv, \"cv_model.onnx\")\n","onnx.save_model(onnx_tfidf, \"tfidf_model.onnx\")\n","onnx.save_model(onnx_model, \"model.onnx\")\n","\n","#Laden der Models\n","model = rt.InferenceSession('knn_model.onnx')\n","\n","#Ausführen des Models\n","output = model.run(None, {'input': X_test_np})"],"metadata":{"id":"Un1ggr57m0tF"},"id":"Un1ggr57m0tF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2.2 Predicitve Model Markup Language (PMML)\n","*https://pypi.org/project/pypmml/*\n","\n","- nicht alle Algorithmen sind transformierbar (z.B. TF-IDF)\n","- unkomplizierte Transformation mit Hilfe von Pipelines"],"metadata":{"id":"EDu2TpMBlBKn"},"id":"EDu2TpMBlBKn"},{"cell_type":"code","source":["from sklearn2pmml import PMMLPipeline\n","from sklearn2pmml import sklearn2pmml\n","import io\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","from pypmml import Model\n","\n","#Daten einlesen\n","data = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","data.pop(\"id\")\n","data = data.sample(frac = 1)\n","\n","#Daten aufteilen\n","X = data[\"text\"]\n","y = data[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n","\n","#Pipeline\n","pipeline = PMMLPipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', KNeighborsClassifier())\n","])\n","\n","pipeline.fit(X_train, y_train)\n","\n","#konvertieren zu pmml\n","sklearn2pmml(pipeline, \"pipeline.pmml\")\n","\n","#Model laden\n","loaded_pipeline = Model.load(\"pipeline.pmml\")\n","\n","#predicted = loaded_pipeline.predict(X_test)"],"metadata":{"id":"nsXh1tTYXF5L"},"id":"nsXh1tTYXF5L","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 Vergleich der Modelle\n","\n","**Neuronales Netz:** Hohe Effizienz und Rechenleistung aufgrund der parallelen Verarbeitung von Daten durch mehrere Schichten. Schwer zu interpretieren und erfordert viel Training und Tuning, um Überanpassung zu vermeiden. (Überanpassung bedeutet, dass sich ein Modell zu sehr an die Trainingsdaten angepasst hat, und mit neuen Daten stark überfordert wäre.)\n","\n","/\n","\n","**K-nearest-neighbor:** Niedrige Effizienz und Rechenleistung, da alle Datenpunkte zur Vorhersage herangezogen werden müssen. Das Modell ist leicht zu interpretieren, aber empfindlich gegenüber Abweichungen bzw. Ungenauigkeit der Datenwerte.\n","\n","/\n","\n","**Naive Bayes:** Hohe Effizienz und niedrige Rechenleistung, da nur einfache statistische Berechnungen durchgeführt werden müssen. Das Modell ist einfach zu interpretieren und gut anpassungsfähig, aber auch empfindlich gegenüber Abweichungen.\n","\n","/\n","\n","**Support Vector Machine:** Hohe Effizienz und Rechenleistung, da nur eine kleine Anzahl von Trainingsdaten benötigt wird. Das Modell ist jedoch schwer zu interpretieren und stark abhängig von der Wahl der Kernel- und Hyperparameter, was es schwieriger macht ein Modell zu trainieren, da die Leistung davon abhängig ist.\n","\n","/\n","\n","**Decision Trees:** Hohe Effizienz, aber niedrige Rechenleistung aufgrund der Baumstruktur und der Entscheidungsregeln. Das Modell ist leicht zu interpretieren und kann gut mit Abweichungen umgehen, aber empfindlich gegenüber Überanpassung.\n","\n","/\n","\n","**Gradient Boost:** Hohe Effizienz und Rechenleistung, da die Entscheidungsregeln iterativ durchgeführt werden. Das Modell ist jedoch schwer zu interpretieren und empfindlich gegenüber Überanpassung.\n","\n","/\n","\n","**FAZIT:** Für große und komplexe Datensätze, würde sich wahrscheinlich ein neurolaes Netz, oder ein Gradient Boost am Besten eignen.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"dmoyUuJCoV6I"},"id":"dmoyUuJCoV6I"},{"cell_type":"markdown","metadata":{"id":"9590c8b6"},"source":["\n","\n","# 5 Glossar\n","\n","### Dimension eines Problems\n","= Dimension der Matrix der Input-Daten\n","\n","### Entscheidungsfunktion/Zielfunktion/Optimierungsfunktion\n","= Algorithmus, der Trainingsdaten auswertet und entscheidet, ob eine Verbesserung oder Verschlechterung stattgefunden hat\n","\n","### Kernel Functions\n","= werden bei SVM verwendet, um die Input-Daten für das Processing in die notwendige Form zu transformieren\n","\n","### Precision\n","= ist das Maß für den Anteil an vorhergesagten positiven Fällen, die auch tatsächlich positiv sind, demensprechend ist eine hohe precision, eine hohe korrekte Vorhersage des Modells\n","\n","### Recall \n","= ist das Maß für die möglichst wichtigsten/relevantesten erfassten Fälle des Modells, ein hoher Recall Wert bedeuted dementsprechend dass das Modell nur wenige relevante Fälle übersehen hat\n","\n","### F1-Score\n","= der F1-Score ist quasi eine Gewichtung aus Recall und Precision, und zeigt wie gut das Modell insgesamt abschneidet\n","\n","### Odds-Ratio\n","= ist ein Maß, welches verwendet wird um einen Zusammenhang zwischen Variablen zu quantifizieren - zeigt wie sich die Wahrscheinlichkeit ändert, wenn etwas mit oder ohne spezielle Bedingung erfüllt wird\n","\n","### Macro AVG\n","- durchschnittswert, der keine Ungleichheit in der Größe der einzelnen Klassen berücksichtigt\n","\n","### Weighted AVG\n","- durchschnittswert, der die Ungleichheit berücksichtigt\n","\n","### Epoch\n","\n","### Batch-Size\n","\n","### Embedding-Size"],"id":"9590c8b6"}],"metadata":{"colab":{"provenance":[{"file_id":"1oNyz6Tph6K1y0cc-AxdZ2AIeDH821EAR","timestamp":1684231666734}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":5}