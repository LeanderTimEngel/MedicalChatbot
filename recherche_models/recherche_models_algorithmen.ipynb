{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":14436,"status":"ok","timestamp":1683987646873,"user":{"displayName":"Susi H","userId":"16263607694578559646"},"user_tz":-120},"id":"ZISDJ2NTnIXx","outputId":"ea140451-00cb-45e3-a0b6-3b417a19e835"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1bce8cc7-af20-4ef1-b664-ccbe20488d89\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1bce8cc7-af20-4ef1-b664-ccbe20488d89\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Symptom2Disease.csv to Symptom2Disease.csv\n"]}],"source":["#Nur f√ºr Colab\n","#Datei hochladen\n","from google.colab import files\n","\n","uploaded = files.upload()"],"id":"ZISDJ2NTnIXx"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":76},"executionInfo":{"elapsed":1546436,"status":"ok","timestamp":1683623821551,"user":{"displayName":"Billy Andersson","userId":"04670995206192713080"},"user_tz":-120},"id":"RH8D8jaSuSM6","outputId":"40f338ff-3073-411a-d8d3-98458aefa3ec"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-dc66160a-1323-4b93-a40d-38ec6838dc11\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-dc66160a-1323-4b93-a40d-38ec6838dc11\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Data1.csv to Data1.csv\n"]}],"source":["#F√ºr Billy\n","from google.colab import files\n","\n","billys_csv = files.upload()"],"id":"RH8D8jaSuSM6"},{"cell_type":"markdown","metadata":{"id":"b6a8812b"},"source":["# 1 Problemstellung\n","\n","- Problemart: supervised Multi-Class Single-Label Text Classification\n","- Supervised = Model wird mit Datens√§tzen trainiert, bei denen das Label bereits bekannt ist\n","- Classification = Zuordnung zu diskreten Werten\n","- Multiclass Single-Label = mehrere Labels, jedem Sample kann nur ein Label zugewiesen werden\n","\n","Andere Arten:\n","- Multi-Label Classification (mehrere Labels, jedem Sampel k√∂nnen mehrere Labels zugewiesen werden)\n","- Binary Classification (z.B. Sentiment-Analysis, nur zwei Label, z.B. positiv und negativ)"],"id":"b6a8812b"},{"cell_type":"markdown","metadata":{"id":"2ec2b10d"},"source":["# 2 Methoden\n","_https://en.wikipedia.org/wiki/Multiclass_classification_<br>\n","_https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=a546f2c88c588a2a46c054f67b39a3ebefdae694_<br>\n","_https://www.ijcsit.com/docs/Volume%204/Vol4Issue4/ijcsit2013040408.pdf_"],"id":"2ec2b10d"},{"cell_type":"markdown","metadata":{"id":"a29e91a9"},"source":["## 2.1 Umwandeln in bin√§re Klassifikation\n","= Problem wird auf mehrere bin√§re Klassifikationen zur√ºckgef√ºhrt"],"id":"a29e91a9"},{"cell_type":"markdown","metadata":{"id":"9d5489ba"},"source":["### 2.1.1 One-vs.-all/One-vs.-rest\n","_https://youtu.be/ZvaELFv5IpM_\n","<figure>\n","    <img src=\"https://www.cc.gatech.edu/classes/AY2016/cs4476_fall/results/proj4/html/jnanda3/svm.png\" alt=\"One-vs.-all Algorithmus\" style=\"width: 400px;\"/>\n","    <br>\n","</figure>\n","<small>Quelle: https://www.cc.gatech.edu/classes/AY2016/cs4476_fall/results/proj4/html/jnanda3/index.html</small>\n","\n","- es wird pro Klasse ein bin√§rer Classifier (z.B. eine logistische Regression oder SVM) trainiert nach dem Prinzip \"einer gegen alle\"\n","    - Beispiel: rot, gr√ºn, blau und gelb\n","        - rot vs. nicht rot\n","        - gr√ºn vs. nicht gr√ºn\n","        - blau vs. nicht blau\n","        - gelb vs. nicht gelb\n","- dann werden auf jeden Punkt im Datensatz alle Classifier angewendet\n","- der Classifier, der die gr√∂√üte Confidence hat, wird gew√§hlt, um den Punkt zu klassifizieren\n","- wird bei gro√üer Anzahl an Klassen ineffizient"],"id":"9d5489ba"},{"cell_type":"markdown","metadata":{"id":"64e58da3"},"source":["### 2.1.2 One-vs.-one\n","_https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/_\n","<figure>\n","    <img src=\"https://image.slidesharecdn.com/linearmodelsandmulticlassclassification2-170312171304/75/linear-models-and-multiclass-classification-25-2048.jpg?cb=1667658416\" alt=\"One-vs.-one Algorithmus\" style=\"width: 400px;\"/>\n","    <br>\n","</figure>\n","<small>Quelle: https://www.slideshare.net/Paxcel/binary-and-multi-class-strategies-for-machine-learning</small>\n","\n","- Problem wird in bin√§re Classifier aufgeteilt nach dem Prinzip \"jede Klasse gegen jede\"\n","    - Beispiel: rot, gr√ºn, blau und gelb\n","        - rot vs. gr√ºn\n","        - rot vs. blau\n","        - rot vs. gelb\n","        - gr√ºn vs. blau\n","        - gr√ºn vs. gelb\n","        - blau vs. gelb\n","- jeder Classifier wird nur mit den Datens√§tzen trainiert, die einer der beiden Klassen angeh√∂ren\n","- Anzahl der bin√§ren Klassifikationsprobleme: (N * (N-1))/2\n","- nach Scoring-System wird am Ende eine Klasse bestimmt"],"id":"64e58da3"},{"cell_type":"markdown","metadata":{"id":"e096403e"},"source":["## 2.2 Erweitern der bin√§ren Klassifikation\n","= Classifier, die eigentlich f√ºr bin√§re Klassifikation gedacht sind, aber f√ºr multiclass erweitert werden"],"id":"e096403e"},{"cell_type":"markdown","metadata":{"id":"048b8a28"},"source":["### 2.2.1 Neuronales Netz\n","_https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/video-lecture_\n","- Neuronales Netz nach One-vs.-all: \n","    - NN f√ºr bin√§re Klassifikation hat √ºblicherweise einen Output-Knoten, der bin√§r klassifiziert, stattdessen neuronales Netz mit so vielen Output-Knoten, wie es Klassen gibt\n","    - die Wahrscheinlichkeiten der Output-Knoten sollen addiert 1 ergeben (-> Softmax als Loss-Funktion/Optimierungsfunktion)\n","- Nachteil: verlangt sehr viel mehr Rechenzeit und -power als andere Herangehensweisen"],"id":"048b8a28"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"OiT4nkJxJtVX","outputId":"b0a53aab-7f2d-483e-9175-f844f9c71c9b","executionInfo":{"status":"error","timestamp":1683623830293,"user_tz":-120,"elapsed":404,"user":{"displayName":"Billy Andersson","userId":"04670995206192713080"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-b719fc941c6d>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data1.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'uploaded' is not defined"]}],"source":["import pandas as pd\n","import io\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, LSTM\n","\n","# Load the dataset\n","data = pd.read_csv(io.BytesIO(billys_csv['Data1.csv']))\n","data.pop(\"id\")\n","\n","# Convert diagnosis column to integers\n","diagnosis_dict = {diagnosis: i for i, diagnosis in enumerate(data['label'].unique())}\n","data['label'] = data['label'].apply(lambda x: diagnosis_dict[x])\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n","\n","# Convert the diagnosis labels into a one-hot encoding format\n","num_classes = len(data['label'].unique())\n","y_train = np.eye(num_classes)[y_train]\n","y_test = np.eye(num_classes)[y_test]\n","\n","# Tokenize the symptoms and convert them into sequences\n","max_words = 10000\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(X_train)\n","X_train_sequences = tokenizer.texts_to_sequences(X_train)\n","X_test_sequences = tokenizer.texts_to_sequences(X_test)\n","\n","# Pad the sequences to ensure that they are all the same length\n","max_len = 100\n","X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len)\n","X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len)\n","\n","# Build the neural network model\n","embedding_size = 128\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_size, input_length=max_len))\n","model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))  #bei mehreren Layers noch zus√§tzlich return_sequences=True eintragen\n","#model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","batch_size = 32\n","epochs = 10\n","model.fit(X_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_padded, y_test))\n","\n","# Evaluate the model on the testing set\n","loss, accuracy = model.evaluate(X_test_padded, y_test, batch_size=batch_size)\n","print('Test loss:', loss)\n","print('Test accuracy:', accuracy)"],"id":"OiT4nkJxJtVX"},{"cell_type":"markdown","metadata":{"id":"fa5b2656"},"source":["### 2.2.2 K-nearest-neighbor\n","_https://towardsdatascience.com/multiclass-classification-using-k-nearest-neighbours-ca5281a9ef76_\n","- Abstand des zu klassifizierenden Punktes zu allen seinen Nachbarn wird berechnet\n","- Nachbarn werden nach Abstand sortiert\n","- Punkt wird in die Klasse eingeordnet, der die meisten seiner k-n√§chsten Nachbarn angeh√∂ren\n","- Funktioniert besser bei kleinen Datens√§tzen, wird sonst ineffizient"],"id":"fa5b2656"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3794,"status":"ok","timestamp":1683970328000,"user":{"displayName":"Susi H","userId":"16263607694578559646"},"user_tz":-120},"id":"0256be75","outputId":"12de932a-f7b5-473c-c88f-76af2278cca1"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                 precision    recall  f1-score   support\n","\n","                           Acne       1.00      1.00      1.00        16\n","                      Arthritis       0.91      1.00      0.95        20\n","               Bronchial Asthma       1.00      0.81      0.90        16\n","           Cervical spondylosis       0.95      1.00      0.98        20\n","                    Chicken pox       0.64      0.93      0.76        15\n","                    Common Cold       0.93      1.00      0.97        14\n","                         Dengue       0.71      0.89      0.79        19\n","          Dimorphic Hemorrhoids       1.00      1.00      1.00        19\n","               Fungal infection       0.95      1.00      0.98        21\n","                   Hypertension       0.92      0.92      0.92        13\n","                       Impetigo       1.00      0.83      0.91        12\n","                       Jaundice       0.92      1.00      0.96        11\n","                        Malaria       0.90      1.00      0.95        18\n","                       Migraine       0.95      1.00      0.97        18\n","                      Pneumonia       0.94      1.00      0.97        17\n","                      Psoriasis       0.94      0.75      0.83        20\n","                        Typhoid       0.79      0.55      0.65        20\n","                 Varicose Veins       1.00      0.92      0.96        12\n","                        allergy       0.94      0.88      0.91        17\n","                       diabetes       0.92      1.00      0.96        12\n","                  drug reaction       1.00      0.77      0.87        13\n","gastroesophageal reflux disease       0.95      0.91      0.93        22\n","           peptic ulcer disease       0.93      0.93      0.93        14\n","        urinary tract infection       1.00      0.88      0.94        17\n","\n","                       accuracy                           0.91       396\n","                      macro avg       0.92      0.92      0.92       396\n","                   weighted avg       0.92      0.91      0.91       396\n","\n"]}],"source":["#Vor dem Ausf√ºhren der Zelle muss ganz oben einmal die csv-Datei hochgeladen werden!\n","#Beispiel-Implementierung von K-nearest-neighbor\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import metrics\n","import io\n","\n","#Datei einlesen\n","data = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","data.pop(\"id\")\n","data = data.sample(frac = 1)\n","\n","#Aufteilen in Trainings- und Testdaten\n","X = data[\"text\"]\n","y = data[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n","\n","#Vektorisieren\n","cv = CountVectorizer()\n","X_train_counts = cv.fit_transform(X_train)\n","X_test_counts = cv.transform(X_test)\n","\n","tfidf_transformer = TfidfTransformer()\n","X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n","X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n","\n","#Trainieren\n","knn = KNeighborsClassifier()\n","\n","knn.fit(X_train_tfidf, y_train)\n","\n","#Vorhersagen der Testdaten\n","predicted = knn.predict(X_test_tfidf)\n","\n","#Auswertung\n","print(metrics.classification_report(y_test, predicted))"],"id":"0256be75"},{"cell_type":"markdown","metadata":{"id":"1eff4278"},"source":["### 2.2.3 Naive Bayes\n","_https://youtu.be/EGKeC2S44Rs_\n","- Familie von Algorithmen\n","- Stellen die \"naive\" Annahme, dass die W√∂rter alle voneinander unabh√§ngig sind\n","- Ablauf:\n","    - Datens√§tze werden vektorisiert (Count Vectorizer):\n","        - L√§nge des Vektors = Anzahl der Worte im Vokabular\n","        - Vektor dr√ºckt aus, wie oft jedes Wort des Vokabulars im Datensatz enthalten ist\n","    - Aus den Trainingsdaten wird die Wahrscheinlichkeit f√ºr jede Klasse abgeleitet\n","    - f√ºr jede Klasse wird die Wahrscheinlichkeit von jedem Wort berechnet"],"id":"1eff4278"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":331,"status":"ok","timestamp":1682932927193,"user":{"displayName":"Susi H","userId":"16263607694578559646"},"user_tz":-120},"id":"600e06f0","outputId":"02848d89-6eb9-423d-bb1b-cf1202f986b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 precision    recall  f1-score   support\n","\n","                           Acne       1.00      1.00      1.00        18\n","                      Arthritis       0.82      1.00      0.90        14\n","               Bronchial Asthma       0.89      1.00      0.94        16\n","           Cervical spondylosis       1.00      0.94      0.97        17\n","                    Chicken pox       1.00      0.94      0.97        18\n","                    Common Cold       0.94      0.94      0.94        18\n","                         Dengue       0.86      0.86      0.86        14\n","          Dimorphic Hemorrhoids       0.88      1.00      0.93        14\n","               Fungal infection       0.74      1.00      0.85        14\n","                   Hypertension       1.00      1.00      1.00        12\n","                       Impetigo       0.88      1.00      0.93        14\n","                       Jaundice       0.95      1.00      0.97        19\n","                        Malaria       0.90      1.00      0.95        18\n","                       Migraine       1.00      0.95      0.97        19\n","                      Pneumonia       1.00      0.92      0.96        24\n","                      Psoriasis       0.93      0.68      0.79        19\n","                        Typhoid       0.88      1.00      0.93        14\n","                 Varicose Veins       1.00      1.00      1.00        19\n","                        allergy       1.00      0.75      0.86        20\n","                       diabetes       1.00      0.40      0.57        20\n","                  drug reaction       1.00      0.69      0.81        16\n","gastroesophageal reflux disease       0.75      1.00      0.86        12\n","           peptic ulcer disease       0.44      0.78      0.56         9\n","        urinary tract infection       0.90      1.00      0.95        18\n","\n","                       accuracy                           0.90       396\n","                      macro avg       0.91      0.91      0.90       396\n","                   weighted avg       0.92      0.90      0.90       396\n","\n"]}],"source":["#Vor dem Ausf√ºhren muss ganz oben einmal die Datei hochgeladen werden!\n","#Beispiel f√ºr Implementierung von Naive Bayes Classifier\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn import metrics\n","import io\n","\n","#Datei einlesen\n","data = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","data.pop(\"id\")\n","data = data.sample(frac = 1)\n","\n","#Aufteilen in Trainings- und Testdaten\n","X = data[\"text\"]\n","y = data[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n","\n","#Pipeline erstellen mit Vektorisierung, Transformation und Naive Bayes Classifier\n","text_clf = Pipeline([('vect', CountVectorizer()),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', MultinomialNB()),\n","                     ])\n","\n","#Trainieren\n","text_clf.fit(X_train, y_train)\n","\n","#Vorhersagen der Testdaten\n","predicted = text_clf.predict(X_test)\n","\n","#Auswertung\n","print(metrics.classification_report(y_test, predicted))"],"id":"600e06f0"},{"cell_type":"markdown","metadata":{"id":"1eedbea5"},"source":["### 2.2.4 Support Vector Machine\n","_https://www.baeldung.com/cs/svm-multiclass-classification#multiclass-classification-using-svm_\n","- Urspr√ºnglich designed f√ºr bin√§re Klassifikation, aber auch anwendbar auf Multi-Class Probleme\n","- Grunds√§tzliche Vorgehensweise bei bin√§rer Klassifikation:\n","    - Ziel: Linie (bzw. Ebene) finden, die die Punkte der beiden Klassen am besten trennt (maximiert den Abstand zu den Punkten beider Klassen)\n","    - Die Datens√§tze, die der Linie am n√§chsten sind, sind die Support-Vectors\n","    - Abstand der Support-Vectors zur Linie ist die Margin, die maximiert werden soll\n","    - Die Berechnung der Trennlinie wird mit eine Kernel-Funktion durchgef√ºhrt\n","    - Es gibt verschiedene Kernel-Funktionen, deren Hyperparameter beim Tuning angepasst werden\n","- Zwei Herangehensweisen: \n","    - One-to-one\n","    - One-to-rest\n","- Verf√ºgbar in sklearn\n","- Vorteile:\n","    - Speichereffizient\n","    - anpassbar (spezifische Kernel Functions ausw√§hlbar)\n","    - kommt mit kleinem Trainigsdatensatz aus"],"id":"1eedbea5"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1682933404068,"user":{"displayName":"Susi H","userId":"16263607694578559646"},"user_tz":-120},"id":"D77d4vvGoPTQ","outputId":"11ebe2ea-29bc-4756-b424-24387e791dc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 precision    recall  f1-score   support\n","\n","                           Acne       1.00      1.00      1.00        17\n","                      Arthritis       1.00      1.00      1.00        17\n","               Bronchial Asthma       1.00      1.00      1.00        11\n","           Cervical spondylosis       1.00      1.00      1.00        21\n","                    Chicken pox       1.00      0.93      0.96        14\n","                    Common Cold       0.95      1.00      0.97        18\n","                         Dengue       0.87      1.00      0.93        13\n","          Dimorphic Hemorrhoids       1.00      1.00      1.00        18\n","               Fungal infection       1.00      1.00      1.00        15\n","                   Hypertension       1.00      1.00      1.00        15\n","                       Impetigo       1.00      1.00      1.00        23\n","                       Jaundice       1.00      1.00      1.00        15\n","                        Malaria       1.00      1.00      1.00        14\n","                       Migraine       1.00      1.00      1.00        17\n","                      Pneumonia       1.00      1.00      1.00        18\n","                      Psoriasis       1.00      0.79      0.88        14\n","                        Typhoid       0.94      0.94      0.94        17\n","                 Varicose Veins       0.92      1.00      0.96        12\n","                        allergy       1.00      0.95      0.97        20\n","                       diabetes       0.83      1.00      0.91        15\n","                  drug reaction       1.00      0.82      0.90        17\n","gastroesophageal reflux disease       1.00      1.00      1.00        19\n","           peptic ulcer disease       0.93      1.00      0.96        13\n","        urinary tract infection       1.00      1.00      1.00        23\n","\n","                       accuracy                           0.98       396\n","                      macro avg       0.98      0.98      0.97       396\n","                   weighted avg       0.98      0.98      0.98       396\n","\n"]}],"source":["#Vor dem Ausf√ºhren muss ganz oben einmal die Datei hochgeladen werden!\n","#Beispiel f√ºr die Implementierung von linear Support-Vector-Machine\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.svm import LinearSVC\n","import io\n","\n","#Datei einlesen\n","data = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","data.pop(\"id\")\n","data = data.sample(frac = 1)\n","\n","#Aufteilen in Trainings- und Testdaten\n","X = data[\"text\"]\n","y = data[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n","\n","#Pipeline mit Vektorisierung, Transformation und SVM\n","text_clf = Pipeline([('vect', CountVectorizer()),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', LinearSVC()),\n","                     ])\n","\n","#Trainieren\n","text_clf.fit(X_train, y_train)\n","\n","#Vorhersagen der Testdaten\n","predicted = text_clf.predict(X_test)\n","\n","#Auswertung\n","print(metrics.classification_report(y_test, predicted))"],"id":"D77d4vvGoPTQ"},{"cell_type":"markdown","metadata":{"id":"27942b69"},"source":["### 2.2.5 Decision Trees\n","_https://www.youtube.com/watch?v=ZVR2Way4nwQ_\n","- Model entwickelt einen Entscheidungsbaum anhand der Trainingsdaten\n","- Aufbau des Entscheidungsbaums:\n","    - Alle Datens√§tze sind in der Root\n","    - Alle Bl√§tter enthalten nur noch Datens√§tze von einer Klasse\n","    - Model vergleicht alle m√∂glichen Aufteilungen (Splits)\n","    - Model sucht Split so aus, dass die Entropie in einem Kindknoten m√∂glichst gering ist -> und der Informationgain m√∂glichst gro√ü ist\n","- neue Datens√§tze werden mit Hilfe des Entscheidungsbaums in die entsprechende Klasse eingeordnet"],"id":"27942b69"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":576,"status":"ok","timestamp":1682938351477,"user":{"displayName":"Susi H","userId":"16263607694578559646"},"user_tz":-120},"id":"jPXe6iRZ7iv0","outputId":"f669fac1-2fb4-4cec-86e0-8140d13e16a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 precision    recall  f1-score   support\n","\n","                           Acne       1.00      1.00      1.00        24\n","                      Arthritis       1.00      1.00      1.00        14\n","               Bronchial Asthma       0.85      0.85      0.85        20\n","           Cervical spondylosis       0.92      0.92      0.92        13\n","                    Chicken pox       0.27      0.19      0.22        16\n","                    Common Cold       0.52      0.79      0.63        14\n","                         Dengue       0.36      0.50      0.42        16\n","          Dimorphic Hemorrhoids       1.00      1.00      1.00        15\n","               Fungal infection       0.92      0.67      0.77        18\n","                   Hypertension       1.00      0.73      0.84        11\n","                       Impetigo       0.76      0.76      0.76        17\n","                       Jaundice       1.00      1.00      1.00        16\n","                        Malaria       1.00      0.94      0.97        17\n","                       Migraine       1.00      0.79      0.88        19\n","                      Pneumonia       0.93      0.72      0.81        18\n","                      Psoriasis       0.46      0.46      0.46        13\n","                        Typhoid       0.68      0.93      0.79        14\n","                 Varicose Veins       1.00      0.96      0.98        24\n","                        allergy       0.61      0.69      0.65        16\n","                       diabetes       0.73      0.58      0.65        19\n","                  drug reaction       0.44      0.64      0.52        11\n","gastroesophageal reflux disease       0.62      0.72      0.67        18\n","           peptic ulcer disease       0.79      0.88      0.83        17\n","        urinary tract infection       0.92      0.75      0.83        16\n","\n","                       accuracy                           0.78       396\n","                      macro avg       0.78      0.77      0.77       396\n","                   weighted avg       0.80      0.78      0.78       396\n","\n"]}],"source":["#Vor dem Ausf√ºhren muss ganz oben die Datei einmal hochgeladen werden!\n","#Beispiel f√ºr die Implementierung von Decision Trees\n","import pandas as pd\n","import io\n","from sklearn.model_selection import train_test_split\n","from sklearn import tree\n","from sklearn.pipeline import Pipeline\n","from sklearn import metrics\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","#Datei einlesen\n","data = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","data.pop(\"id\")\n","data = data.sample(frac = 1)\n","\n","#Aufteilen in Trainings- und Testdaten\n","X = data[\"text\"]\n","y = data[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n","\n","#Pipeline mit Vektorisierung, Transformation und Decision Tree\n","text_clf = Pipeline([('vect', CountVectorizer()),\n","                     ('tfidf', TfidfTransformer()),\n","                     ('clf', tree.DecisionTreeClassifier()),\n","                     ])\n","\n","#Trainieren\n","text_clf.fit(X_train, y_train)\n","\n","#Vorhersagen der Testdaten\n","predicted = text_clf.predict(X_test)\n","\n","#Auswertung\n","print(metrics.classification_report(y_test, predicted))"],"id":"jPXe6iRZ7iv0"},{"cell_type":"markdown","source":["###2.2.6 Gradient Boost\n","\n","https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n","\n","* basiert auf Ensemble Methoden\n","* kombiniert mehrere schwache Lernalgorithmen zu einer starken; jeder Algorithmus wird nacheinander trainiert und versucht den schw√§cheren zu verbessern\n","\n","\n","*   beginnt mit Basismodel von beliebigen Algorithmus, der f√ºr Klassifikation oder Regression geeignet ist\n","    \n","    - neues Model wird erstellt und korrigiert die Fehler des Basismodels; Gradientenabstieg wird verwendet um das neue Modell zu trainieren\n","\n","    - neues trainiertes Modell wird dem Basismodell hinzugef√ºgt, um ein neues Essemble Modell zu erstellen -> das wird oft wiederholt\n","  \n","\n","\n","*   sehr leistungsf√§higer Algorithmus und gut f√ºr gro√üe Datens√§tze, vor Allem weil das Modell durch die ganzen Wiederholungen sehr pr√§zise wird\n","\n","    - ist aber auch sehr rechenintensiv\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"P4H47EIVnXVo"},"id":"P4H47EIVnXVo"},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import io\n","\n","# Einlesen des Datensatzes als CSV-Datei\n","df = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","df.pop(\"id\")\n","\n","# Aufteilen der Features und Labels\n","X = df['text']\n","y = df['label']\n","\n","# Aufteilen des Datensatzes in Trainings- und Testdaten\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","#Pipeline mit Vektorisierung, Transformation und Gradient Boost Classifier\n","text_clf = Pipeline([('vect', CountVectorizer()),\n","                     ('gb_clf', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)),\n","                     ])\n","\n","# Trainieren des Modells\n","text_clf.fit(X_train, y_train)\n","\n","# Vorhersage auf Testdaten machen\n","y_pred = text_clf.predict(X_test)\n","\n","# Berechnen der Genauigkeit\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"id":"f-5KQRytreiV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683574195805,"user_tz":-120,"elapsed":14339,"user":{"displayName":"Susi H","userId":"16263607694578559646"}},"outputId":"b1455d07-636d-4d1c-9037-f61925d5af9a"},"id":"f-5KQRytreiV","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.89\n"]}]},{"cell_type":"markdown","metadata":{"id":"6a40b7b7"},"source":["## 2.3 Hierarchische Klassifikation\n","= Aufteilung in Ober- und Unterklassen in einem Baum, f√ºr die Aufteilung an den einzelnen Knoten wird meist ein bin√§rer Classifier genutzt"],"id":"6a40b7b7"},{"cell_type":"markdown","source":["# 3 Persistenz\n","*https://scikit-learn.org/stable/model_persistence.html*\n","- wichtig bei Text-Klassifizierung: sowohl das Model, als auch der/die Vectorizer m√ºssen gespeichert werden\n","- es muss au√üerdem sichergestellt werden, dass das Model in die gleiche scikit-learn Version geladen wird, in der es auch gespeichert wurde"],"metadata":{"id":"IOFLFkcOYc__"},"id":"IOFLFkcOYc__"},{"cell_type":"markdown","source":["## 3.1 Python-spezifische Serialisierung\n","\n","- unterst√ºtzen alle auch das Speichern gesamter Pipelines"],"metadata":{"id":"QndeWrAdjrTP"},"id":"QndeWrAdjrTP"},{"cell_type":"markdown","source":["### 3.1.1 Speichern mit Pickle"],"metadata":{"id":"JKOHrw5PZFoD"},"id":"JKOHrw5PZFoD"},{"cell_type":"code","source":["#In colab wird das Model nur in dem Sitzungsspeicher abgespeichert, also ist das Model nur f√ºr die aktuelle Sitzung persistent (kann aber nat√ºrlich aus Colab runtergeladen oder ein anderes hochgeladen werden)\n","#Aber wenn man es in JupyterNotebook o.√Ñ. benutzt wird es lokal im selben Verzeichnis gespeichert\n","import pickle\n","\n","#Speichern\n","with open('file.pkl', 'wb') as f:\n","  pickle.dump(model, f)\n","\n","#Laden\n","with open('file.pkl', 'rb') as f:\n","    loaded_model = pickle.load(f)"],"metadata":{"id":"oFdm2NaYavHy"},"id":"oFdm2NaYavHy","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.1.2 Speichern mit joblib\n","\n","- effizienter als Pickle, besonders bei gro√üen Arrays"],"metadata":{"id":"2DQMp1YjZLnU"},"id":"2DQMp1YjZLnU"},{"cell_type":"code","source":["from joblib import dump, load\n","\n","#Speichern\n","dump(model, 'file.joblib')\n","\n","#Laden\n","loaded_model = load('file.joblib')"],"metadata":{"id":"bjxVSw10d_XO"},"id":"bjxVSw10d_XO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.1.3 Speichern mit skops\n","*https://skops.readthedocs.io/en/stable/persistence.html*\n","\n","- sicherer als joblib und Pickle\n","- l√§dt nur Models, die keine \"untrusted types\" enthalten"],"metadata":{"id":"hM56PEY6ffvy"},"id":"hM56PEY6ffvy"},{"cell_type":"code","source":["import skops.io as sio\n","\n","#Speichern\n","sio.dump(model, 'file.skops')\n","\n","#√úberpr√ºfung der untrusted types\n","unknown_types = sio.get_untrusted_types(file=\"file.skops\")\n","print(unknown_types)\n","\n","#Laden des Models nach √úberpr√ºfung der untrusted types\n","loaded_model = sio.load(\"file.skops\", trusted=unknown_types)"],"metadata":{"id":"TSb2ly8-f5qW"},"id":"TSb2ly8-f5qW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 Kompatible Formate\n","\n","- wenn ein Model √ºber unterschiedliche Architekturen und Umgebungen hinaus genutzt werden soll, sollte das Model am besten in eines der \"interoperable formats\" exportiert werden\n","- z.B. wenn die Prediction in einer anderen Umgebung gemacht werden soll als das Training"],"metadata":{"id":"42Kx-QCVj_PA"},"id":"42Kx-QCVj_PA"},{"cell_type":"markdown","source":["### 3.2.1 Open Neural Network Exchange (ONNX)\n","*http://onnx.ai/sklearn-onnx/*\n","- bin√§re Serialisierung des Models\n","- Vorgaben/Einschr√§nkungen:\n","  - F√ºr das Konvertieren muss der vektorisierte Datensatz von einer \"sparse matrix\" in eine \"dense matrix\" umgewandelt werden.\n","  - Nachteil dabei ist, dass sparse matrices effizienter (besonders was den Speicher angeht) sind als dense matrices.\n","  - Au√üerdem kann das Konvertieren unter Umst√§nden zu einem Verlust bei der accuracy f√ºhren, wird aber bei unserer Problemstellung und unseren Models vermutlich kaum eine Rolle spielen.\n","  - Kann aber ausgeglichen werden, indem das Model auf die Verwendung von dense matrices getuned wird\n","- Alle Schritte von Vektorisierung bis eigentliches Model m√ºssen f√ºr ein best√§ndiges Output konvertiert werden\n","  - dabei ist es leider nicht so einfach, die alle in einem Model zu speichern\n","  - deshalb m√ºssen sie vermutlich alle einzeln konvertiert werden, das bedeutet, dass besonders sorgf√§ltig darauf geachtet werden muss, dass Reihenfolge und Formate nach dem Laden eingehalten werden\n","- Das Konvertieren von gesamten Pipelines ist auch m√∂glich, aber dann ist meist immer noch eine Umwandlung der sparse Matrix in eine dense Matrix n√∂tig, was bei einer Pipeline nur schwer umsetzbar ist\n"],"metadata":{"id":"dJUkCcUhk7dF"},"id":"dJUkCcUhk7dF"},{"cell_type":"code","source":["from onnx import helper, TensorProto, numpy_helper\n","import onnx\n","import skl2onnx\n","import onnxruntime as rt\n","from onnxconverter_common.data_types import StringTensorType, Int64TensorType, FloatTensorType\n","\n","#Beispiel f√ºr das Transformieren eines Count Vectorizers in ein dense numpy array\n","X_train_np = X_train_tfidf.toarray()\n","X_test_np = X_test_tfidf.toarray()\n","\n","#Vectorizer konvertieren\n","initial_type = [('input', StringTensorType([None,]))]\n","onnx_cv = skl2onnx.convert_sklearn(cv, initial_types=initial_type)\n","\n","#Transformer konvertieren\n","initial_type = [('input', FloatTensorType([None, X_train_np.shape[1]]))]\n","onnx_tfidf = skl2onnx.convert_sklearn(tfidf_transformer, initial_types=initial_type)\n","\n","#Model konvertieren\n","initial_type = [('input', FloatTensorType([None, X_train_np.shape[1]]))]\n","onnx_model = skl2onnx.convert_sklearn(model, initial_types=initial_type)\n","\n","#konvertierte Models speichert\n","onnx.save_model(onnx_cv, \"cv_model.onnx\")\n","onnx.save_model(onnx_tfidf, \"tfidf_model.onnx\")\n","onnx.save_model(onnx_model, \"model.onnx\")\n","\n","#Laden der Models\n","model = rt.InferenceSession('knn_model.onnx')\n","\n","#Ausf√ºhren des Models\n","output = model.run(None, {'input': X_test_np})"],"metadata":{"id":"Un1ggr57m0tF"},"id":"Un1ggr57m0tF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.2.2 Predicitve Model Markup Language (PMML)\n","*https://pypi.org/project/pypmml/*\n","\n","- nicht alle Algorithmen sind transformierbar (z.B. TF-IDF)\n","- unkomplizierte Transformation mit Hilfe von Pipelines"],"metadata":{"id":"EDu2TpMBlBKn"},"id":"EDu2TpMBlBKn"},{"cell_type":"code","source":["from sklearn2pmml import PMMLPipeline\n","from sklearn2pmml import sklearn2pmml\n","import io\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","from pypmml import Model\n","\n","#Daten einlesen\n","data = pd.read_csv(io.BytesIO(uploaded['Symptom2Disease.csv']))\n","data.pop(\"id\")\n","data = data.sample(frac = 1)\n","\n","#Daten aufteilen\n","X = data[\"text\"]\n","y = data[\"label\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n","\n","#Pipeline\n","pipeline = PMMLPipeline([\n","    ('vect', CountVectorizer()),\n","    ('clf', KNeighborsClassifier())\n","])\n","\n","pipeline.fit(X_train, y_train)\n","\n","#konvertieren zu pmml\n","sklearn2pmml(pipeline, \"pipeline.pmml\")\n","\n","#Model laden\n","loaded_pipeline = Model.load(\"pipeline.pmml\")\n","\n","#predicted = loaded_pipeline.predict(X_test)"],"metadata":{"id":"nsXh1tTYXF5L"},"id":"nsXh1tTYXF5L","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 Vergleich der Modelle\n","\n","**Neuronales Netz:** Hohe Effizienz und Rechenleistung aufgrund der parallelen Verarbeitung von Daten durch mehrere Schichten. Schwer zu interpretieren und erfordert viel Training und Tuning, um √úberanpassung zu vermeiden. (√úberanpassung bedeutet, dass sich ein Modell zu sehr an die Trainingsdaten angepasst hat, und mit neuen Daten stark √ºberfordert w√§re.)\n","\n","/\n","\n","**K-nearest-neighbor:** Niedrige Effizienz und Rechenleistung, da alle Datenpunkte zur Vorhersage herangezogen werden m√ºssen. Das Modell ist leicht zu interpretieren, aber empfindlich gegen√ºber Abweichungen bzw. Ungenauigkeit der Datenwerte.\n","\n","/\n","\n","**Naive Bayes:** Hohe Effizienz und niedrige Rechenleistung, da nur einfache statistische Berechnungen durchgef√ºhrt werden m√ºssen. Das Modell ist einfach zu interpretieren und gut anpassungsf√§hig, aber auch empfindlich gegen√ºber Abweichungen.\n","\n","/\n","\n","**Support Vector Machine:** Hohe Effizienz und Rechenleistung, da nur eine kleine Anzahl von Trainingsdaten ben√∂tigt wird. Das Modell ist jedoch schwer zu interpretieren und stark abh√§ngig von der Wahl der Kernel- und Hyperparameter, was es schwieriger macht ein Modell zu trainieren, da die Leistung davon abh√§ngig ist.\n","\n","/\n","\n","**Decision Trees:** Hohe Effizienz, aber niedrige Rechenleistung aufgrund der Baumstruktur und der Entscheidungsregeln. Das Modell ist leicht zu interpretieren und kann gut mit Abweichungen umgehen, aber empfindlich gegen√ºber √úberanpassung.\n","\n","/\n","\n","**Gradient Boost:** Hohe Effizienz und Rechenleistung, da die Entscheidungsregeln iterativ durchgef√ºhrt werden. Das Modell ist jedoch schwer zu interpretieren und empfindlich gegen√ºber √úberanpassung.\n","\n","/\n","\n","**FAZIT:** F√ºr gro√üe und komplexe Datens√§tze, w√ºrde sich wahrscheinlich ein neurolaes Netz, oder ein Gradient Boost am Besten eignen.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"dmoyUuJCoV6I"},"id":"dmoyUuJCoV6I"},{"cell_type":"markdown","metadata":{"id":"9590c8b6"},"source":["\n","\n","# 5 Glossar\n","\n","### Dimension eines Problems\n","= Dimension der Matrix der Input-Daten\n","\n","### Entscheidungsfunktion/Zielfunktion/Optimierungsfunktion\n","= Algorithmus, der Trainingsdaten auswertet und entscheidet, ob eine Verbesserung oder Verschlechterung stattgefunden hat\n","\n","### Kernel Functions\n","= werden bei SVM verwendet, um die Input-Daten f√ºr das Processing in die notwendige Form zu transformieren\n","\n","### Precision\n","= ist das Ma√ü f√ºr den Anteil an vorhergesagten positiven F√§llen, die auch tats√§chlich positiv sind, demensprechend ist eine hohe precision, eine hohe korrekte Vorhersage des Modells\n","\n","### Recall \n","= ist das Ma√ü f√ºr die m√∂glichst wichtigsten/relevantesten erfassten F√§lle des Modells, ein hoher Recall Wert bedeuted dementsprechend dass das Modell nur wenige relevante F√§lle √ºbersehen hat\n","\n","### F1-Score\n","= der F1-Score ist quasi eine Gewichtung aus Recall und Precision, und zeigt wie gut das Modell insgesamt abschneidet\n","\n","### Odds-Ratio\n","= ist ein Ma√ü, welches verwendet wird um einen Zusammenhang zwischen Variablen zu quantifizieren - zeigt wie sich die Wahrscheinlichkeit √§ndert, wenn etwas mit oder ohne spezielle Bedingung erf√ºllt wird\n","\n","### Macro AVG\n","- durchschnittswert, der keine Ungleichheit in der Gr√∂√üe der einzelnen Klassen ber√ºcksichtigt\n","\n","### Weighted AVG\n","- durchschnittswert, der die Ungleichheit ber√ºcksichtigt\n","\n","### Epoch\n","\n","### Batch-Size\n","\n","### Embedding-Size"],"id":"9590c8b6"}],"metadata":{"colab":{"provenance":[{"file_id":"1oNyz6Tph6K1y0cc-AxdZ2AIeDH821EAR","timestamp":1684231666734}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":5}